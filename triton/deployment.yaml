apiVersion: apps/v1
kind: Deployment
metadata:
  name: triton-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: triton
  template:
    metadata:
      labels:
        app: triton
    spec:

      initContainers:
      - name: load-heavy-onnx-model
        image: python:3.11
        command:
          - sh
          - -c
          - |
            set -e

            echo "[init] Installing onnx"
            pip install --no-cache-dir --prefer-binary onnx

            echo "[init] Generating ONNX model"
            python <<'EOF'
            import onnx
            from onnx import helper, TensorProto
            import os

            input_name = "input"
            input_tensor = helper.make_tensor_value_info(
              input_name, TensorProto.FLOAT, [1, 1024, 1024]
            )

            nodes = []
            initializers = []
            last = input_name

            for i in range(8):
              w_name = f"w{i}"
              out_name = f"x{i}"

              w = helper.make_tensor(
                name=w_name,
                data_type=TensorProto.FLOAT,
                dims=[1024, 1024],
                vals=[0.001] * (1024 * 1024),
              )

              node = helper.make_node(
                "MatMul",
                inputs=[last, w_name],
                outputs=[out_name],
              )

              nodes.append(node)
              initializers.append(w)
              last = out_name

            output_tensor = helper.make_tensor_value_info(
              last, TensorProto.FLOAT, [1, 1024, 1024]
            )

            graph = helper.make_graph(
              nodes,
              "heavy-matmul",
              [input_tensor],
              [output_tensor],
              initializer=initializers,
            )

            model = helper.make_model(graph)
            onnx.checker.check_model(model)

            os.makedirs("/models/heavy/1", exist_ok=True)
            onnx.save(model, "/models/heavy/1/model.onnx")

            with open("/models/heavy/config.pbtxt", "w") as f:
              f.write(
                'name: "heavy"\n'
                'platform: "onnxruntime_onnx"\n'
                'max_batch_size: 1\n'
                f'input [ {{ name: "{input_name}" data_type: TYPE_FP32 dims: [1024, 1024] }} ]\n'
                f'output [ {{ name: "{last}" data_type: TYPE_FP32 dims: [1024, 1024] }} ]\n'
              )

            print("[init] Model generation complete")
            EOF
        volumeMounts:
        - name: model-repo
          mountPath: /models

      containers:
      - name: triton
        image: nvcr.io/nvidia/tritonserver:25.11-py3
        args:
          - tritonserver
          - --model-repository=/models
        ports:
        - containerPort: 8000
        - containerPort: 8001
        - containerPort: 8002
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
            nvidia.com/gpu: 1
          limits:
            memory: "4Gi"
            nvidia.com/gpu: 1
        volumeMounts:
        - name: model-repo
          mountPath: /models

      volumes:
      - name: model-repo
        emptyDir: {}
